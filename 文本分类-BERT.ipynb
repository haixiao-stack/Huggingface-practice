{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774ddcb9-9ced-4f75-afb5-b38e654133e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import thirdparty\n",
    "from thirdparty import *\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizerFast,DistilBertForSequenceClassification, DistilBertConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModel,AutoConfig,AutoModelForSequenceClassification\n",
    "from safetensors.torch import load_file\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dbb7b6-6dc1-46e8-a147-092c6e9588e5",
   "metadata": {},
   "source": [
    "# 语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa4e3ee-0ece-428e-9b78-31f2031be3e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "text = [\"中国的首都是北京\"]\n",
    "model_dir=\"/root/autodl-tmp/model/\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(os.path.join(model_dir,\"bert-base-chinese\"))\n",
    "bert_model = AutoModel.from_pretrained(os.path.join(model_dir,\"bert-base-chinese\"))\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(os.path.join(model_dir,\"bart4csc-base-chinese\"))\n",
    "bart_model = AutoModel.from_pretrained(os.path.join(model_dir,\"bart4csc-base-chinese\"))\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(os.path.join(model_dir,\"gpt2\"))\n",
    "gpt_model = AutoModel.from_pretrained(os.path.join(model_dir,\"gpt2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a439458f-8e20-49f6-9ca8-d66e5f910944",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "[[101, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102]]\n",
      "[CLS] 中 国 的 首 都 是 北 京 [SEP]\n",
      "bart\n",
      "[[101, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102]]\n",
      "[CLS] 中 国 的 首 都 是 北 京 [SEP]\n",
      "gpt\n",
      "[[40792, 32368, 121, 21410, 165, 99, 244, 32849, 121, 42468, 44293, 245, 12859, 105]]\n",
      "中国的首都是北京\n"
     ]
    }
   ],
   "source": [
    "print (\"bert\")\n",
    "inputs = bert_tokenizer(text, return_tensors='pt',padding=True)['input_ids']\n",
    "print (inputs.tolist())\n",
    "print (bert_tokenizer.decode(inputs.tolist()[0]))\n",
    "print (\"bart\")\n",
    "inputs = bart_tokenizer(text, return_tensors='pt',padding=True)['input_ids']\n",
    "print (inputs.tolist())\n",
    "print (bart_tokenizer.decode(inputs.tolist()[0]))\n",
    "print (\"gpt\")\n",
    "inputs = gpt_tokenizer(text, return_tensors='pt')['input_ids']\n",
    "print (inputs.tolist())\n",
    "print (gpt_tokenizer.decode(inputs.tolist()[0]))\n",
    "#['input_ids'] 所有语言模型共有的，['token_type_ids']表示字符属于第几句话，是BERT和BART特有的\n",
    "#，['attention_mask'] 是否开启掩码 0 代表被遮盖 1 代表不遮盖\n",
    "#分词与模型配套使用,不同分词器分出的结果是不一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea67cf1-5a11-4012-859a-c1c6a674dde9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "text = [\"关云长温酒斩华雄\"]\n",
    "inputs = bert_tokenizer(text, return_tensors='pt',padding=True)['input_ids']\n",
    "output = bert_model(inputs)\n",
    "print(output.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "385ba7dd-1e8a-45e8-9544-d9d2d34d5a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# last_hidden_state：模型最后一层的隐藏状态，常用于各种下游任务。\n",
    "# pooler_output：池化层的输出，通常用于句子级别的任务（如分类）。\n",
    "# past_key_values：缓存的键和值，用于加速生成任务。\n",
    "# hidden_states：每一层的隐藏状态，用于深入分析或特殊任务。\n",
    "# attentions：注意力权重，显示模型的注意力机制。\n",
    "# cross_attentions：跨注意力权重，在某些架构（如 Transformer 编码器-解码器）中使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e128ef1-b0c2-420b-8a32-abbd3525dad0",
   "metadata": {},
   "source": [
    "# 生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7014c9-1e88-429a-92e7-d8bc630d3ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_index={\"体育\":0,\"娱乐\":1,\"家居\":2,\"彩票\":3,\"房产\":4,\"教育\":5,\"时尚\":6 , \"时政\":7 , \"星座\":8 , \"游戏\":9,  \"社会\":10,\"科技\":11}\n",
    "def read_imdb_split(path):\n",
    "    split_dir = Path(path)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in label_index.keys():\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text(encoding='utf-8', errors='ignore'))\n",
    "            labels.append(label_index[label_dir])\n",
    "    return texts, labels\n",
    "#读取原始数据\n",
    "train_texts, train_labels = read_imdb_split('/root/autodl-tmp/data/text-classification/train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01b1a6a0-afb9-43cc-a6f2-cf79897893da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109966\n",
      "《异形大战铁血战士》配置要求公布\n",
      "　　由Rebellion开发、世嘉发行的科幻射击游戏《异形VS铁血战士》(Aliens vs. Predator)现已确定发售日期。根据世嘉的《异形VS铁血战士》官方网页的信息，本作将于2010年2月19日上市，对应PC、PS3和Xbox 360平台。\n",
      "　　这部根据电影改编的游戏也同样存在着三方势力，一方是人类海军陆战队，一方是异形，还有一方是铁血战士，游戏中玩家可以选择的是海军陆战队或者是铁血战士甚至是异形。本作提供单人模式和多人模式。\n",
      "　　最低配置要求：\n",
      "　　系统：Windows 7/ XP/Vista\n",
      "　　内存：1 GB System RAM (XP)/ 2 GB System RAM (Vista)\n",
      "　　处理器：3.2 GHz Intel Pentium 4/Athlon 64 3000+ 或更高\n",
      "　　显卡：支持DirectX 9.0c 128 MB RAM 显存(NVIDIA 6600 或更高， ATI X1600 或更高)\n",
      "　　推荐配置要求：\n",
      "　　系统：Windows 7/ XP/Vista\n",
      "　　处理器：Intel Core 2 Duo E6400 或更高\n",
      "　　内存：2 GB System RAM\n",
      "　　显卡：支持DirectX 9.0c 512 MB RAM (NVIDIA 8800 系列， ATI HD2900 PRO 或更高)\n",
      "\n",
      "\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))\n",
    "# 观察文本和标签数据\n",
    "index=80998\n",
    "print(train_texts[index])\n",
    "print(train_labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6ab09ad-9b22-403b-acaf-f1ebcb2361dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#转化成标准格式\n",
    "train_dataset=Dataset.from_dict({'inputs':train_texts,'labels':train_labels},split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1020310b-e4ae-4bbd-8a63-6c0da6fed48b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': '女孩边大呼救命边跑到13楼跳下身亡\\n\\u3000\\u3000据都市一时间报道 长沙八一路的天佑大厦前天发生一起意外，一名24岁的女子从13楼坠楼。目击者称这名女子本来住在十五楼，事发时一边大叫救命，一边跑到十三楼的一家私家菜房，然后纵身跳下。\\n\\u3000\\u3000据死者父亲介绍，前天早上曾接到女儿的电话，电话那边很急，“要我们赶快过来。”\\n\\u3000\\u3000据私家菜房的老板说，“当时这个女孩冲进来，借我们的电话向她爸爸求救，结果话没说完一下子冲到窗边就跳下去了。”目前，私家菜房的厨师已到公安部门配合调查。而死者在天佑大厦的开房记录也被警方带走。\\n\\u3000\\u3000酒店方面表示，由于刚开张不久，酒店没有启动监控设备，目前警方正在调查这起坠楼事件。\\n\\n', 'labels': 10}\n"
     ]
    }
   ],
   "source": [
    "# 观察文本和标签数据\n",
    "index = 96500\n",
    "print(train_dataset[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483d0e6-0bde-4849-9eb4-87250f46b6b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 进行分词操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad4181c6-04ae-47d7-bbb7-35b1e10de951",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd436d918d8498886b441fa9e839926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/109966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name=\"distilbert-base-uncased\"# bert-base-chinese\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(model_dir,model_name))\n",
    "def tokenize_dataset(tokenizer, dataset, max_len):\n",
    "    #把读取出来的文本数据，转成标准输入格式\n",
    "    def convert_to_features(example_batch):\n",
    "        src_texts = []\n",
    "        trg_texts = []\n",
    "        for terms in zip(example_batch['inputs'],example_batch['labels']):\n",
    "            src_texts.append(terms[0])\n",
    "            trg_texts.append(terms[1])\n",
    "        input_encodings = tokenizer.batch_encode_plus(src_texts,truncation=True,padding='max_length',max_length=max_len)\n",
    "        encodings = {'input_ids': input_encodings['input_ids'],'labels': trg_texts}\n",
    "        return encodings\n",
    "    dataset = dataset.map(convert_to_features, batched=True)\n",
    "    dataset = dataset.remove_columns(['inputs'])\n",
    "    return dataset\n",
    "train_dataset=tokenize_dataset(tokenizer, train_dataset,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75ff153b-9a31-4c34-86c0-360267d38b38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': 9, 'input_ids': [101, 1639, 100, 100, 100, 100, 1016, 1640, 21469, 2278, 100, 100, 1772, 100, 100, 100, 100, 100, 100, 1864, 19413, 1772, 100, 100, 1639, 100, 100, 100, 100, 1016, 1640, 100, 100, 21469, 2278, 1000, 100, 100, 1006, 18695, 1007, 1000, 1989, 100, 21469, 2278, 100, 1740, 100, 1802, 100, 100, 1989, 100, 100, 100, 100, 1862, 1018, 100, 1802, 100, 1881, 100, 1810, 1957, 1006, 2380, 3927, 1007, 1635, 1802, 100, 1782, 1006, 6671, 1007, 1635, 100, 100, 100, 1006, 13858, 1007, 1796, 1810, 100, 1006, 7328, 1007, 1989, 100, 100, 100, 100, 1940, 100, 5385, 100, 102]}\n",
      "[CLS] 《 [UNK] [UNK] [UNK] [UNK] 2 》 dlc [UNK] [UNK] 公 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 日 ea 公 [UNK] [UNK] 《 [UNK] [UNK] [UNK] [UNK] 2 》 [UNK] [UNK] dlc \" [UNK] [UNK] ( retaliation ) \" ， [UNK] dlc [UNK] 一 [UNK] 地 [UNK] [UNK] ， [UNK] [UNK] [UNK] [UNK] 新 4 [UNK] 地 [UNK] 林 [UNK] 大 道 ( park avenue ) 、 地 [UNK] 区 ( transit ) 、 [UNK] [UNK] [UNK] ( shipyard ) 和 大 [UNK] ( compound ) ， [UNK] [UNK] [UNK] [UNK] 花 [UNK] 800 [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "index = 88500\n",
    "print(train_dataset[index])\n",
    "print(tokenizer.decode(train_dataset[index]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60ac49-1551-4b5b-812e-56f573db2c44",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d97e7f14-12e1-41f0-a128-7ce6956daa56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /root/autodl-tmp/model/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(os.path.join(model_dir,model_name),num_labels=len(label_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60608ac-4222-4136-a9a6-c278bbef8c46",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0f036c3-d1db-441c-bf4f-82368c4ddaaa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1290' max='1290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1290/1290 09:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.617800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.553500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.460600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.415700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.342100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.313200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.285200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1290, training_loss=0.42683168455611825, metrics={'train_runtime': 560.7596, 'train_samples_per_second': 588.306, 'train_steps_per_second': 2.3, 'total_flos': 8536820954414400.0, 'train_loss': 0.42683168455611825, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./result/text-classification',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=256,  # batch size per device during training\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.001,               # strength of weight decay\n",
    "    logging_dir='./log/text-classification',            # directory for storing logs\n",
    "    logging_steps=100,            \n",
    "    save_steps=500,                  # Save checkpoints every 500 steps\n",
    "    save_total_limit=2,              # Only keep the last 2 checkpoints\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated   Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03804937-ba94-46dd-881b-1cb9b7e339e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./finetune-model/bert/\")\n",
    "torch.save(model,\"./finetune-model/bert/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3f025-2c75-4431-ab95-89fdb1a743df",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c79f09f5-bae2-4a14-9200-972a887a2f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_texts, test_labels = read_imdb_split('/root/autodl-tmp/data/text-classification/test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e519cea-b5e7-4ef2-84e6-d091e656bace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index  100, 预测标签家居, 实际标签 体育\n",
      "index  200, 预测标签娱乐, 实际标签 娱乐\n",
      "index  300, 预测标签家居, 实际标签 家居\n",
      "index  400, 预测标签彩票, 实际标签 彩票\n",
      "index  500, 预测标签房产, 实际标签 房产\n",
      "index  600, 预测标签教育, 实际标签 教育\n",
      "index  700, 预测标签时尚, 实际标签 时尚\n",
      "index  800, 预测标签时政, 实际标签 时政\n",
      "index  900, 预测标签时尚, 实际标签 星座\n",
      "index 1000, 预测标签游戏, 实际标签 游戏\n",
      "index 1100, 预测标签社会, 实际标签 社会\n",
      "index 1200, 预测标签科技, 实际标签 科技\n",
      "正确率 0.8608333333333333\n"
     ]
    }
   ],
   "source": [
    "device=get_device()\n",
    "classifier = pipeline(\"sentiment-analysis\",model=torch.load(\"./finetune-model/bert/pytorch_model.bin\").to(device),tokenizer=os.path.join(model_dir,model_name)) \n",
    "right=count=0\n",
    "index_label=dict([[a,b] for b,a in label_index.items()])\n",
    "i = 0\n",
    "for text,label in zip(test_texts,test_labels):\n",
    "    #预测出来的结果\n",
    "    result=classifier(text[0:512])\n",
    "    label2=index_label[int(result[0]['label'].split(\"_\")[1])]\n",
    "    #真实结果\n",
    "    label=index_label[label]\n",
    "    if label2==label:\n",
    "        right+=1\n",
    "    i+=1\n",
    "    if(i%100==0):\n",
    "        print(\"index{:5d}, 预测标签{}, 实际标签 {}\".format(i,label2,label))\n",
    "    count+=1\n",
    "print (\"正确率\",right/count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
