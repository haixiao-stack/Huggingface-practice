# Huggingface-practice

- 文本分类：利用BERT模型，将num_labels指定为10，在分类新闻数据集进行训练，最终达到0.86准确率

- 文本纠错：利用BART模型，将中文错误文本当作input，与之对应的正确文本当作标签，对模型进行微调，

  模型最后的输出更加准确

- 图像分类：利用ViT模型，将classifier层的输出维度改为100并在cifar-100数据集上进行linear-probe微调，最终达到0.83正确率
- 图文匹配：利用CLIP模型，将vit-model部分的参数冻结，利用COCO数据集部分图片匹配的中文句子做训练集，最后用中文句子测试图片，微调后模型对两句中文的区分度从0.3变为0.94
- 看图说话，图片问答：分别利用BLIP-2和BLIP完成上述两个任务
